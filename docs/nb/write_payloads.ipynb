{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sndaq.reader import SN_PayloadReader\n",
    "from sndaq.writer import SN_PayloadWriter, construct_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read one SN payload from existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supernova@157144851073427008[dom 88ce18beda45 clk 03103a410001 scalerData*652\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../data/sndata-spts-209058_001-010'\n",
    "data_file = 'sn_209058_000001_239420_529095.dat'\n",
    "with SN_PayloadReader(os.path.join(data_dir, data_file)) as r:\n",
    "    pay = next(r)\n",
    "print(pay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Payload construction & Writing Payloads\n",
    "Test `sndaq.writer.SN_PayloadWriter` and `sndaq.writer.construct_payloads` via the following steps\n",
    "1. Write an existing SN payload to file\n",
    "2. Construct a brand new custom SN payload\n",
    "3. Write this custom payload to file\n",
    "4. Read this file to check for equivalence\n",
    "\n",
    "__Notes__\n",
    " - Times chosen below are arbitrary for sake of example, variable names containing `utime` reflect the time of an event in UTC, since the start of the year, measured in 0.1 ns\n",
    " - Scaler counts follow a poisson distribution, `scaler_lambda` was arbitrariy selected\n",
    " - Dictionary with keys matching keyword args of construct_payload may be unpacked via **\n",
    "   - These values can just be passed as arguments to `construct_payload`, I just like using dicts for example code\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './data/test.dat'\n",
    "with SN_PayloadWriter(test_file, overwrite=True) as w:\n",
    "    ### Test writer against existing payload\n",
    "    w.write(pay)\n",
    "    \n",
    "    ### Test Construction of custom payload\n",
    "    \n",
    "    utime = 123456789  # Time of payload collection\n",
    "    launch_utime = 120006539  # Time of DOM power-on\n",
    "    n_scalers = 670\n",
    "    scaler_lambda = 0.8  # \n",
    "\n",
    "    \n",
    "    custom_pay_dict = {\n",
    "        'utime': utime, \n",
    "        'dom_id': 0xe9fed8c717dd,  # DOM [1-6] Chutes_and_Ladders, value equivalent to 257280767891421\n",
    "        'domclock': (utime - launch_utime)//250,  # Integer number of 25 ns clock cycles since DOM activation\n",
    "        'scalers': np.random.poisson(scaler_lambda, size=n_scalers),\n",
    "        'keep_data': True\n",
    "    }\n",
    "    \n",
    "    custom_payload = construct_payload(**custom_pay_dict)\n",
    "    \n",
    "    ### Test writer against custom payload\n",
    "    w.write(custom_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check via print(payload) before reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Expected Custom Payload __str__ (Sanity check only, Not a test of equivalence) ===\n",
      "Supernova@123456789[dom e9fed8c717dd clk 0000000035e9 scalerData*670]\n",
      "\n",
      "Supernova@123456789[dom e9fed8c717dd clk 0000000035e9 scalerData*670\n"
     ]
    }
   ],
   "source": [
    "print('=== Expected Custom Payload __str__ (Sanity check only, Not a test of equivalence) ===')\n",
    "print(\"Supernova@{0:d}[dom {1:012x} clk {2:012x} scalerData*{3:d}]\\n\".format(\n",
    "    utime, custom_pay_dict['dom_id'], custom_pay_dict['domclock'], n_scalers))\n",
    "print(custom_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check via print(payload) after reading from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supernova@157144851073427008[dom 88ce18beda45 clk 03103a410001 scalerData*652\n",
      "Supernova@123456789[dom e9fed8c717dd clk 0000000035e9 scalerData*670\n",
      "\n",
      "=== Expected Custom Payload __str__ (Sanity check only, Not a test of equivalence) ===\n",
      "Supernova@123456789[dom e9fed8c717dd clk 0000000035e9 scalerData*670]\n"
     ]
    }
   ],
   "source": [
    "with SN_PayloadReader(test_file) as r:\n",
    "    while r.nrec < 2:\n",
    "        pay = next(r)\n",
    "        print(pay)\n",
    "        \n",
    "print('\\n=== Expected Custom Payload __str__ (Sanity check only, Not a test of equivalence) ===')\n",
    "print(\"Supernova@{0:d}[dom {1:012x} clk {2:012x} scalerData*{3:d}]\".format(\n",
    "    utime, custom_pay_dict['dom_id'], custom_pay_dict['domclock'], n_scalers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sndata for Unit-testing\n",
    "\n",
    "Real sndata.dat files are constructed taking into consideration a few principles\n",
    "1. Every payload in a .dat file is arranged in ascending time, and every .dat file is also ordered in ascending times\n",
    "2. Every sequential payload for a particular DOM has a utime that corresponds to the scaler readout immediately following the last scaler in the previous file.\n",
    " - Scalers are collected over a period of $2^{16}$ clock cycles, so in units 0.1 ns, that is $2^{16} \\times 250 = 16384000$, or 1.6384 ms.\n",
    " - In brief, each scaler is collected 1.6384 ms after the previous scaler.\n",
    " - The payload `utime` and `domclock` fields refers to the time and no. of DOM clock cycles at the first scaler in the payload\n",
    " - For DOM $i$ with first payload $p_{i,0}$, `utime` $t_{i,0}$ and $n_{i,0}$ scalers, the second payload for that DOM in the file $p_{i,1}$ will have `utime` $t_{i,1} = t_{i,0} + n_{i,0} \\times (2^{16} \\times 250)$\n",
    "3. Real sndata files collect ~190 MB of data before a new file is created\n",
    "4. Every DOM in the run configuration issues payload $n$ before any DOM issues paylaod $n+1$\n",
    "\n",
    "Committing large files is not recommended, but having some files would be useful for unit testing.\n",
    "\n",
    "A data set would look something like the following\n",
    "\n",
    " - Small size (<25MB). \n",
    "   - File size is prop. to duration and number of DOMs. Having a longer duration is more important for testing, so a test set might only need one or several strings of the detector. \n",
    " - Adheres to the file format/principles above\n",
    " - Contains specific features (Unless it is intended to represent background) that can affect trigger significance, to test trigger formation algorithm\n",
    "   - Occasionally in real data, the one or more of the conditions will not be met, and SNDAQ will handle it somewhat quietly (usually) For the sake of testing, we should assume all of these conditions to be met, unless we are making a test set to specifically examine how SNDAQ handles these circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sndaq.detector import Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i3 = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doms': [221612503003558, 261790137316497, 220496300680470, 170083391137477, 207507749111472, 59697474087023, 113721244932466, 280538975565061, 133055326516044, 25503628325740]}\n"
     ]
    }
   ],
   "source": [
    "# Random valid ids\n",
    "np.random.seed(42)  # Randomly selected results for the seed 42, this allows the selection to be reproduced\n",
    "valid_ids = np.random.choice(i3.dom_table['mbid'], size = 10)\n",
    "dict_valid_ids = {'doms': [mbid for mbid in valid_ids]} \n",
    "print(dict_valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doms': [221612503003614, 261790137316553, 220496300680470, 170083391137477, 207507749111472, 59697474087079, 113721244932522, 280538975565061, 133055326516044, 25503628325740]}\n",
      "\n",
      "Modified Ids:\n",
      "Index |        Valid ID |      Invalid ID\n",
      "    0 | 221612503003610 | 221612503003614\n",
      "    1 | 261790137316549 | 261790137316553\n",
      "    5 |  59697474087075 |  59697474087079\n",
      "    6 | 113721244932518 | 113721244932522\n"
     ]
    }
   ],
   "source": [
    "# Introduce some invalid ids (by modifying valid ids)\n",
    "np.random.seed(43)\n",
    "mask = np.random.randint(0,2,size=valid_ids.shape).astype(np.bool)\n",
    "mask = np.invert(mask)\n",
    "\n",
    "mixed_ids = np.array(valid_ids)  # This creates a seperate instance of the variable valid_ids\n",
    "mixed_ids[mask] += 4\n",
    "\n",
    "dict_mixed_ids = {'doms': [mbid for mbid in mixed_ids]} \n",
    "print(dict_mixed_ids)\n",
    "\n",
    "print('\\nModified Ids:')\n",
    "print('{0:5s} | {1:>15s} | {2:>15s}'.format('Index', 'Valid ID', 'Invalid ID'))\n",
    "for idx, valid_mbid, invalid_mbid in zip(mask.nonzero()[0], valid_ids[mask], mixed_ids[mask]):\n",
    "    print(f'{idx:>5d} |{valid_mbid:>16d} |{invalid_mbid:>16d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str': [1, 52]}\n"
     ]
    }
   ],
   "source": [
    "# Random valid strings\n",
    "np.random.seed(48)  \n",
    "valid_str = np.random.randint(1,86, size=2)\n",
    "dict_valid_str = {'str': [mbid for mbid in valid_str]} \n",
    "# 'doms': [mbid for mbid in valid_ids],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str': [1, 99]}\n"
     ]
    }
   ],
   "source": [
    "# Introduce some invalid strings (by modifying valid ids)\n",
    "mixed_str = np.array(valid_str)\n",
    "mixed_str[-1] = 99\n",
    "dict_mixed_str = {'str': [mbid for mbid in mixed_str]} \n",
    "print(dict_mixed_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doms': [221612503003558, 261790137316497, 220496300680470, 170083391137477, 207507749111472, 59697474087023, 113721244932466, 280538975565061, 133055326516044, 25503628325740, 207271527053912, 20579555797555, 76224551397868, 167080981203282, 31155393599125, 171972502366323, 20579555797555, 138631493291764, 266169248464026, 125366650734220], 'str': [1]}\n"
     ]
    }
   ],
   "source": [
    "# Account for duplicate entries on strings and DOM ids\n",
    "np.random.seed(42)  # Randomly selected results for the seed 42, this allows the selection to be reproduced\n",
    "dom_str = 1\n",
    "valid_ids = np.random.choice(i3.dom_table['mbid'], size = 10)\n",
    "str_ids = np.random.choice(i3.dom_table['mbid'][i3.dom_table['str']==dom_str], size = 10)\n",
    "valid_ids = np.append(valid_ids, str_ids)\n",
    "dict_duplicates = {\n",
    "    'doms': [mbid for mbid in valid_ids],\n",
    "    'str': [1]\n",
    "}\n",
    "print(dict_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}